{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c568434c",
      "metadata": {
        "id": "c568434c"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zCq8QLg4NCSH",
        "outputId": "836b2750-64d0-4d67-c51f-a9232d2d8a85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zCq8QLg4NCSH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "753d063b",
      "metadata": {
        "id": "753d063b"
      },
      "outputs": [],
      "source": [
        "# Import Necessary Library\n",
        "import gc\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f749a0c",
      "metadata": {
        "id": "9f749a0c"
      },
      "outputs": [],
      "source": [
        "pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e1cb209",
      "metadata": {
        "id": "8e1cb209"
      },
      "outputs": [],
      "source": [
        "import splitfolders\n",
        "input_folder = 'C:/Users/nusra/OneDrive/Desktop/Data2/Dataset'\n",
        "\n",
        "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.7, .2, .1)`.\n",
        "splitfolders.ratio(input_folder, output='C:/Users/nusra/OneDrive/Desktop/Data2/Evaluation', seed=42, ratio=(.7, .2, .1), group_prefix=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba9508c1",
      "metadata": {
        "id": "ba9508c1"
      },
      "outputs": [],
      "source": [
        "# Load dataset in three different directory for further processing\n",
        "train_directory = \"C:/Users/nusra/OneDrive/Desktop/Data2/Evaluation/train\"\n",
        "test_directory = \"C:/Users/nusra/OneDrive/Desktop/Data2/Evaluation/test\"\n",
        "validation_directory = \"C:/Users/nusra/OneDrive/Desktop/Data2/Evaluation/val\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "931b6067",
      "metadata": {
        "id": "931b6067"
      },
      "outputs": [],
      "source": [
        "# Load dataset in train, validation, and test directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e87bc126",
      "metadata": {
        "id": "e87bc126"
      },
      "outputs": [],
      "source": [
        "DATADIR = train_directory\n",
        "CATEGORIES = [\"Monkeypox\",\"Other\"]\n",
        "training_data = []\n",
        "\n",
        "IMG_SIZE = 100\n",
        "\n",
        "def create_training_data():\n",
        "  for category in CATEGORIES:\n",
        "    path = os.path.join(DATADIR,category)\n",
        "    class_num = CATEGORIES.index(category)\n",
        "    for img in tqdm(os.listdir(path)):\n",
        "      try:\n",
        "        img_array = cv2.imread(os.path.join(path,img))  # convert to array\n",
        "        new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "        training_data.append([img_array, class_num])  # add this to our training_data\n",
        "      except Exception as e:  # in the interest in keeping the output clean...\n",
        "        print(e)\n",
        "\n",
        "create_training_data()\n",
        "print(len(training_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b864583",
      "metadata": {
        "id": "9b864583"
      },
      "outputs": [],
      "source": [
        "DATADIR = validation_directory\n",
        "CATEGORIES = [\"Monkeypox\",\"Other\"]\n",
        "validation_data = []\n",
        "\n",
        "IMG_SIZE = 100\n",
        "\n",
        "def create_validation_data():\n",
        "  for category in CATEGORIES:\n",
        "    path = os.path.join(DATADIR,category)\n",
        "    class_num = CATEGORIES.index(category)\n",
        "    for img in tqdm(os.listdir(path)):\n",
        "      try:\n",
        "        img_array = cv2.imread(os.path.join(path,img))  # convert to array\n",
        "        new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "        validation_data.append([img_array, class_num])  # add this to our training_data\n",
        "      except Exception as e:  # in the interest in keeping the output clean...\n",
        "        print(e)\n",
        "\n",
        "create_validation_data()\n",
        "print(len(validation_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dc7e4f0",
      "metadata": {
        "id": "2dc7e4f0"
      },
      "outputs": [],
      "source": [
        "DATADIR = test_directory\n",
        "CATEGORIES = [\"Monkeypox\",\"Other\"]\n",
        "testing_data = []\n",
        "\n",
        "IMG_SIZE = 100\n",
        "\n",
        "def create_testing_data():\n",
        "  for category in CATEGORIES:\n",
        "    path = os.path.join(DATADIR,category)\n",
        "    class_num = CATEGORIES.index(category)\n",
        "    for img in tqdm(os.listdir(path)):\n",
        "      try:\n",
        "        img_array = cv2.imread(os.path.join(path,img))  # convert to array\n",
        "        new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "        testing_data.append([img_array, class_num])  # add this to our training_data\n",
        "      except Exception as e:  # in the interest in keeping the output clean...\n",
        "        print(e)\n",
        "\n",
        "create_testing_data()\n",
        "print(len(testing_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3778e9d8",
      "metadata": {
        "id": "3778e9d8"
      },
      "outputs": [],
      "source": [
        "# Apply random suffleing and normalization\n",
        "\n",
        "random.shuffle(training_data)\n",
        "random.shuffle(validation_data)\n",
        "random.shuffle(testing_data)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for features, label in training_data:\n",
        "  X.append(features)\n",
        "  y.append(label)\n",
        "\n",
        "X_train = np.array(X)\n",
        "X_train = X_train/255.0\n",
        "y_train = np.array(y)\n",
        "\n",
        "\n",
        "X_v = []\n",
        "y_v = []\n",
        "\n",
        "for features, label in validation_data:\n",
        "  X_v.append(features)\n",
        "  y_v.append(label)\n",
        "\n",
        "X_val = np.array(X_v)\n",
        "X_val = X_val/255.0\n",
        "y_val = np.array(y_v)\n",
        "\n",
        "\n",
        "X_t = []\n",
        "y_t = []\n",
        "\n",
        "for features, label in testing_data:\n",
        "    X_t.append(features)\n",
        "    y_t.append(label)\n",
        "\n",
        "X_test = np.array(X_t)\n",
        "X_test = X_test/255.0\n",
        "y_test = np.array(y_t)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "print(len(X_train), \": \",len(X_test), \": \", len(X_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab649e27",
      "metadata": {
        "id": "ab649e27"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e95f190",
      "metadata": {
        "id": "6e95f190"
      },
      "outputs": [],
      "source": [
        "# Define the number of clients, number of clients help to improve the performance.\n",
        "# Here I set only 5 due to lake of computational resources\n",
        "\n",
        "def create_clients(image_list, label_list, num_clients=5, initial='clients'):\n",
        "  #create a list of client names\n",
        "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
        "    data = list(zip(image_list, label_list))\n",
        "    #shard data and place at each client\n",
        "    size = len(data)//num_clients\n",
        "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
        "    #number of clients must equal number of shards\n",
        "    assert(len(shards) == len(client_names))\n",
        "    return {client_names[i] : shards[i] for i in range(len(client_names))}\n",
        "clients = create_clients(X_train, y_train, num_clients=5, initial='client')\n",
        "\n",
        "def batch_data(data_shard, bs=16):\n",
        "    #seperate shard into data and labels lists\n",
        "    data, label = zip(*data_shard)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
        "    return dataset.shuffle(len(label)).batch(bs)\n",
        "\n",
        "#process and batch the training data for each client\n",
        "clients_batched = dict()\n",
        "co = 0\n",
        "for (client_name, data) in clients.items():\n",
        "    co+=1\n",
        "    clients_batched[client_name] = batch_data(data)\n",
        "\n",
        "#process and batch the test set\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
        "clients_batched"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87b64a49",
      "metadata": {
        "id": "87b64a49"
      },
      "outputs": [],
      "source": [
        "del(X_train)\n",
        "del(y_train)\n",
        "del(training_data)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ab8f243",
      "metadata": {
        "id": "1ab8f243"
      },
      "outputs": [],
      "source": [
        "# Import library for CNN model\n",
        "from keras.layers import Input, Conv2D, Dense, Flatten, MaxPool2D\n",
        "from keras.layers import Activation, Add, BatchNormalization, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e86a332f",
      "metadata": {
        "id": "e86a332f"
      },
      "outputs": [],
      "source": [
        "class SimpleModel:\n",
        "    def build(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(activation ='relu', input_shape = (150,150,3), filters=256, kernel_size=(3, 3), padding=\"SAME\", strides=(1, 1)))\n",
        "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(activation ='relu',filters=128, kernel_size=(3, 3), padding=\"SAME\", strides=(1,1)))\n",
        "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(activation ='relu',filters=64, kernel_size=(3, 3), padding=\"SAME\", strides=(1,1)))\n",
        "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(activation ='relu',filters=32, kernel_size=(3, 3), padding=\"SAME\", strides=(1,1)))\n",
        "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512, activation='relu'))\n",
        "\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "\n",
        "        model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adfc9c0b",
      "metadata": {
        "id": "adfc9c0b"
      },
      "outputs": [],
      "source": [
        "lr = 0.001\n",
        "loss='binary_crossentropy'\n",
        "# loss = 'categorical_crossentropy'\n",
        "metrics = [keras.metrics.binary_accuracy]\n",
        "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr)\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "852697ab",
      "metadata": {
        "id": "852697ab"
      },
      "outputs": [],
      "source": [
        "# from classification_models.tfkeras import Classifiers\n",
        "\n",
        "def weight_scalling_factor(clients_trn_data, client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "    #get the bs\n",
        "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
        "    #first calculate the total training data points across clinets\n",
        "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
        "    print(global_count)\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
        "    return local_count/global_count\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "    return avg_grad\n",
        "\n",
        "def test_local_model(X_test, Y_test,  model, comm_round):\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    #logits = model.predict(X_test, batch_size=100)\n",
        "    logits = model.predict(X_test)\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    print('comm_round: {} | local_acc: {:.3%} | local_loss: {}'.format(comm_round, acc, loss))\n",
        "    return acc, loss\n",
        "\n",
        "def test_global_model(X_test, Y_test,  model, comm_round):\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    #logits = model.predict(X_test, batch_size=100)\n",
        "    logits = model.predict(X_test)\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
        "    return acc, loss\n",
        "\n",
        "smlp_global = SimpleModel()\n",
        "global_model = tf.keras.models.load_model('C:/Users/nusra/OneDrive/Desktop/Dataset/model/global_model_0.h5')\n",
        "global_model = None\n",
        "if global_model == None:\n",
        "    global_model = smlp_global.build()\n",
        "else:\n",
        "    print(\"path found\")\n",
        "\n",
        "global_model.summary()\n",
        "\n",
        "local_model = smlp_global.build()\n",
        "local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b1b169",
      "metadata": {
        "id": "12b1b169"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "comms_round =4  # get the global model's weights - will serve as the initial weights for all local models\n",
        "count = 0\n",
        "for comm_round in range(0, comms_round):\n",
        "    count += 1\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    #initial list to collect local model weights after scalling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    #randomize client data - using keys\n",
        "    client_names= list(clients_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "\n",
        "    #loop through each client and create new local model\n",
        "    count = 0\n",
        "\n",
        "    for client in tqdm(client_names, desc = 'Progress Bar'):\n",
        "        # for client in client_names:\n",
        "        local_model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=[keras.metrics.binary_accuracy])\n",
        "        #print(local_model.summary())\n",
        "        #set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "        #fit local model with client's data\n",
        "        #local_model.fit(clients_batched[client], batch_size = 64, epochs=1+(45//(comm_round+1)), verbose=1, validation_data=(X_val, y_val), shuffle = False)\n",
        "        local_model.fit(clients_batched[client], batch_size = 64, epochs=25, verbose=1, validation_data=(X_val, y_val), shuffle = False)\n",
        "        test_local_model(X_test, y_test, local_model, comm_round)\n",
        "        #scale the model weights and add to list\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "        #clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "\n",
        "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    #global_model.save('C:/Users/nusra/OneDrive/Desktop/Dataset/model/global_model_' +count+ '.h5')\n",
        "#     global_model.save('C:/Users/nusra/OneDrive/Desktop/Dataset/model/global_model_' + str(count) + '.h5')\n",
        "\n",
        "    print(\"done\")\n",
        "\n",
        "    #update global model\n",
        "    global_model.set_weights(average_weights)\n",
        "\n",
        "    #test global model and print out metrics after each communications round\n",
        "    for(X_test, Y_test) in test_batched:\n",
        "        global_acc, global_loss = test_global_model(X_test, Y_test, global_model, comm_round)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa597acd",
      "metadata": {
        "id": "fa597acd"
      },
      "outputs": [],
      "source": [
        "# Plot the confusion matrix. Set Normalize = True/False\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "  plt.title(title)\n",
        "  plt.colorbar()\n",
        "\n",
        "  tick_marks = np.arange(len(classes))\n",
        "  plt.xticks(tick_marks, classes, rotation=45)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "\n",
        "  if normalize:\n",
        "      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "      cm = np.around(cm, decimals=2)\n",
        "      cm[np.isnan(cm)] = 0.0\n",
        "      print(\"Normalized confusion matrix\")\n",
        "  else:\n",
        "      print('Confusion matrix, without normalization')\n",
        "  thresh = cm.max() / 2.\n",
        "\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "      plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "# CATEGORIES = [\"benign\", \"malignant\"]\n",
        "# target_names=[\"ADI\", \"BACK\",\"DEB\", \"LYM\",\"MUC\", \"MUS\",\"NORM\", \"STR\",\"TUM\"]\n",
        "\n",
        "predict_x=global_model.predict(X_test)\n",
        "y_pred=np.argmax(predict_x,axis=1)\n",
        "y_testreport=np.argmax(y_test,axis=1)\n",
        "print('Classification Report')\n",
        "print(classification_report(y_testreport, y_pred, target_names = [\"Monkeypox\",\"Other\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fd78636",
      "metadata": {
        "id": "0fd78636"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "print('Confusion Matrix')\n",
        "cm = confusion_matrix(y_testreport, y_pred)\n",
        "plot_confusion_matrix(cm, [\"Monkeypox\",\"Other\"], title='Confusion Matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da7f468d",
      "metadata": {
        "id": "da7f468d"
      },
      "outputs": [],
      "source": [
        "#PLot fractional incorrect misclassifications\n",
        "class_names = [\"Monkeypox\",\"Other\"]\n",
        "incorr_fraction = 1 - np.diag(cm) / np.sum(cm, axis=1)\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "plt.bar(np.arange(2), incorr_fraction)\n",
        "plt.xlabel('True Label')\n",
        "plt.ylabel('Fraction of incorrect predictions')\n",
        "plt.xticks(np.arange(2), class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1532af4",
      "metadata": {
        "id": "d1532af4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Add this code after the classification report\n",
        "fpr, tpr, thresholds = roc_curve(y_testreport, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed070f61",
      "metadata": {
        "id": "ed070f61"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}